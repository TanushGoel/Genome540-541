For each transcription factor, I chose to train a small transformer neural network. I used TensorFlow rather than PyTorch because I was more familiar with the framework. Also, instead of one-hot encoding, I found it better to byte-pair encode the sequence and then use the transformer's token and positional embedding instead. I found that the best accuracy with low computational cost/runtime was with three transformer blocks and then three fully-connected layers at the end. The models do try to maximize the area under the ROC curve (AUROC) for the validation set - I did use the validation TFs to help choose model parameters, saving the weights of the models with the best validation accuracy for each transcription factor.